{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ezafe_blstm_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNRqu6pWILdpUnY5xBMRUr7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a1d7b36c245548f49c9e0911a938de82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df738b2e52264f01840b6a02262828b9",
              "IPY_MODEL_73228fa008f248ecbae985d422f1c440",
              "IPY_MODEL_56d0b9a5bbe24437a26c52350c8cd488"
            ],
            "layout": "IPY_MODEL_cb750fdcf4634b5e83b0b7fb9d623a26"
          }
        },
        "df738b2e52264f01840b6a02262828b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ff32ea16d064aa7b61a677fdc35b972",
            "placeholder": "​",
            "style": "IPY_MODEL_4710444771c74f63a129713929237066",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json: "
          }
        },
        "73228fa008f248ecbae985d422f1c440": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52f0b7a183934bcb9b249d7c144d65a2",
            "max": 24459,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3837e205f090494f816b3fbd6a546699",
            "value": 24459
          }
        },
        "56d0b9a5bbe24437a26c52350c8cd488": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1f6e4e5fc2b4b909531452555e9995e",
            "placeholder": "​",
            "style": "IPY_MODEL_a9b1b5ed07734af1b14db4d370433209",
            "value": " 142k/? [00:00&lt;00:00, 3.77MB/s]"
          }
        },
        "cb750fdcf4634b5e83b0b7fb9d623a26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ff32ea16d064aa7b61a677fdc35b972": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4710444771c74f63a129713929237066": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52f0b7a183934bcb9b249d7c144d65a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3837e205f090494f816b3fbd6a546699": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e1f6e4e5fc2b4b909531452555e9995e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9b1b5ed07734af1b14db4d370433209": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahdiEsrafili/pert/blob/master/ezafe_blstm_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fk2oqX4CHbHt",
        "outputId": "9b4a7473-a863-4286-d5a5-21f4b4a6d6de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnebXvZ2HiRB",
        "outputId": "4d674d6a-7e6b-4928-94e8-a4ebfbf4a743"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stanza\n",
            "  Downloading stanza-1.3.0-py3-none-any.whl (432 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 33.5 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30 kB 16.9 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 51 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 61 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████                          | 81 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 102 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 112 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 122 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 143 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 153 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 163 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 174 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 184 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 194 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 204 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 215 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 225 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 235 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 245 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 256 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 266 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 276 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 286 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 296 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 307 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 317 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 327 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 337 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 348 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 358 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 368 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 378 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 389 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 399 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 409 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 419 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 430 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 432 kB 8.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from stanza) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stanza) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from stanza) (1.10.0+cu111)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanza) (3.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stanza) (1.21.5)\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 56.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->stanza) (4.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2.10)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=ef8cf517999970f4c7230cf77061f42e2e6a55efd7e444d611c587681fdfd0aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji, stanza\n",
            "Successfully installed emoji-1.7.0 stanza-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "stanza.download('fa')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132,
          "referenced_widgets": [
            "a1d7b36c245548f49c9e0911a938de82",
            "df738b2e52264f01840b6a02262828b9",
            "73228fa008f248ecbae985d422f1c440",
            "56d0b9a5bbe24437a26c52350c8cd488",
            "cb750fdcf4634b5e83b0b7fb9d623a26",
            "7ff32ea16d064aa7b61a677fdc35b972",
            "4710444771c74f63a129713929237066",
            "52f0b7a183934bcb9b249d7c144d65a2",
            "3837e205f090494f816b3fbd6a546699",
            "e1f6e4e5fc2b4b909531452555e9995e",
            "a9b1b5ed07734af1b14db4d370433209"
          ]
        },
        "id": "K67Qbr_4Hs3b",
        "outputId": "ba3b5adc-38f6-46fc-b742-d90bdc3983b0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1d7b36c245548f49c9e0911a938de82"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-16 07:55:09 INFO: Downloading default packages for language: fa (Persian)...\n",
            "2022-04-16 07:55:10 INFO: File exists: /root/stanza_resources/fa/default.zip.\n",
            "2022-04-16 07:55:12 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_pos = stanza.Pipeline(lang='fa', processors='tokenize,mwt,pos')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vY9qy_5sIAw_",
        "outputId": "3dec791b-5e75-4797-c4b6-349cb02760b7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-16 07:55:12 INFO: Loading these models for language: fa (Persian):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | perdt   |\n",
            "| mwt       | perdt   |\n",
            "| pos       | perdt   |\n",
            "=======================\n",
            "\n",
            "2022-04-16 07:55:12 INFO: Use device: gpu\n",
            "2022-04-16 07:55:12 INFO: Loading: tokenize\n",
            "2022-04-16 07:55:21 INFO: Loading: mwt\n",
            "2022-04-16 07:55:21 INFO: Loading: pos\n",
            "2022-04-16 07:55:21 INFO: Done loading processors!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/fastText.git\n",
        "% cd fastText\n",
        "!  pip install .\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EloiwuwfIB7P",
        "outputId": "ab223c22-732a-49c4-dc87-fe00486f6a83"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fastText'...\n",
            "remote: Enumerating objects: 3930, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 3930 (delta 28), reused 42 (delta 11), pack-reused 3854\u001b[K\n",
            "Receiving objects: 100% (3930/3930), 8.33 MiB | 17.85 MiB/s, done.\n",
            "Resolving deltas: 100% (2445/2445), done.\n",
            "/content/fastText\n",
            "Processing /content/fastText\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.9.2-py2.py3-none-any.whl (213 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (1.21.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3139519 sha256=7e4c6c11a786418c6803b64013ad763e2169ee90ae70715abc6f7df71dbb22c1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4e3z_8b8/wheels/22/04/6e/b3aba25c1a5845898b5871a0df37c2126cb0cc9326ad0c08e7\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.9.2\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "% cd fastText\n",
        "!./download_model.py fa\n",
        "% cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Fw2SkiaID03",
        "outputId": "91fd7afe-094e-4a2d-94e5-11b3a8ca032d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/fastText\n",
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz\n",
            " (100.00%) [==================================================>]\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data"
      ],
      "metadata": {
        "id": "1GvRlOd7IFwn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run this or below cell\n",
        "!cp drive/MyDrive/دیتاست\\ بیجن\\ خان/test_clean.tsv data/test_clean.tsv"
      ],
      "metadata": {
        "id": "rtOY4RPkOrtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip drive/MyDrive/دیتاست\\ بیجن\\ خان/test_data.zip -d data\n",
        "!unzip drive/MyDrive/دیتاست\\ بیجن\\ خان/train_data.zip -d data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPeQzq74III-",
        "outputId": "c1ce6133-a291-4df6-ceaf-b1f5e0a80742"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  drive/MyDrive/دیتاست بیجن خان/test_data.zip\n",
            "  inflating: data/test_data.txt      \n",
            "Archive:  drive/MyDrive/دیتاست بیجن خان/train_data.zip\n",
            "  inflating: data/train_data.txt     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_pos = stanza.Pipeline(lang='fa', processors='tokenize,mwt,pos')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IByGIyi0IMLm",
        "outputId": "e9432e46-88ab-4bfc-a041-c14c01567417"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-16 07:55:21 INFO: Loading these models for language: fa (Persian):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | perdt   |\n",
            "| mwt       | perdt   |\n",
            "| pos       | perdt   |\n",
            "=======================\n",
            "\n",
            "2022-04-16 07:55:21 INFO: Use device: gpu\n",
            "2022-04-16 07:55:21 INFO: Loading: tokenize\n",
            "2022-04-16 07:55:21 INFO: Loading: mwt\n",
            "2022-04-16 07:55:21 INFO: Loading: pos\n",
            "2022-04-16 07:55:21 INFO: Done loading processors!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def pos_tager(text):\n",
        "  doc = nlp_pos(text)\n",
        "  pos = [word.xpos for sent in doc.sentences for word in sent.words if word.text not in ['ش','شان','م', 'مان', 'ند','ست','یت', 'تان', 'ت','اش']]\n",
        "  # pos_ = [(word.text , word.upos, word.xpos) for sent in doc.sentences for word in sent.words ]\n",
        "  # print(pos_)\n",
        "  return pos"
      ],
      "metadata": {
        "id": "TGxCnhOfIO4W"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "r8ZQFOE_IP13"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_consist(pos_tag, kasre_tag, sent):\n",
        "  l1 = len(pos_tag)\n",
        "  l2 = len(kasre_tag)\n",
        "  l3 = len(sent.split())\n",
        "  if l1!=l2:\n",
        "    return False\n",
        "  if l1!=l3:\n",
        "    return False\n",
        "  if l3!=l2:\n",
        "    return False\n",
        "  return True"
      ],
      "metadata": {
        "id": "I70JzE8PIUg5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data/test_data.txt', encoding='utf-8') as f:\n",
        "    data = f.readlines()\n",
        "sents = list()\n",
        "kasre_tags = list()\n",
        "temp_sent = ''\n",
        "temp_tags = list()\n",
        "pos_tags = list()\n",
        "for i, line in tqdm(enumerate(data)):\n",
        "    # if i>10000: break\n",
        "    try:\n",
        "        word, ez = line.split()\n",
        "        word += ' '\n",
        "        temp_sent += word\n",
        "        temp_tags.append(ez)\n",
        "        if word in ['. ', '# ']:\n",
        "            pos_tag = pos_tager(temp_sent)\n",
        "            ok = check_consist(pos_tag, temp_tags, temp_sent)\n",
        "            if ok:\n",
        "              sents.append(temp_sent) \n",
        "              kasre_tags.append(temp_tags)\n",
        "              pos_tags.append(pos_tag)\n",
        "            temp_sent = ''\n",
        "            temp_tags = list()\n",
        "    except:\n",
        "        pos_tag = pos_tager(temp_sent)\n",
        "        ok = check_consist(pos_tag, temp_tags, temp_sent)\n",
        "        if ok:\n",
        "          sents.append(temp_sent) \n",
        "          kasre_tags.append(temp_tags)\n",
        "          pos_tags.append(pos_tag)\n",
        "        temp_sent = ''\n",
        "        temp_tags = list()   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrWMK6ZPIVaF",
        "outputId": "e63efa6d-5041-49cc-c9a3-20d777ff3b38"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "970it [00:01, 1102.52it/s]/usr/local/lib/python3.7/dist-packages/stanza/models/common/beam.py:86: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  prevK = bestScoresId // numWords\n",
            "1525970it [19:16, 1319.64it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sents), len(kasre_tags), len(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWuilPW2IYWW",
        "outputId": "079a6ea1-7181-4be3-b5f5-619f6023ea1d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(56132, 56132, 56132)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raws = list()\n",
        "\n",
        "for i in range(len(pos_tags)):\n",
        "  for j in range(len(pos_tags[i])):\n",
        "    raw = f'{sents[i].split()[j]}\\t{pos_tags[i][j]}\\t{kasre_tags[i][j]}'\n",
        "    raws.append(raw)\n",
        "  raws.append('\\n')"
      ],
      "metadata": {
        "id": "IyetfYuTIdCu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raws_text = '\\n'.join(raws)\n",
        "with open('data/test_clean.tsv', 'w') as f:\n",
        "  f.write(raws_text)"
      ],
      "metadata": {
        "id": "Bw0bUhwYIeTW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp data/test_clean.tsv drive/MyDrive/دیتاست\\ بیجن\\ خان/"
      ],
      "metadata": {
        "id": "6uJsLaiROct1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run this or bellow cell\n",
        "!cp drive/MyDrive/fastText/cc.fa.300.vec fastText/"
      ],
      "metadata": {
        "id": "q-mzML_PRHLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fasttext import load_model\n",
        "\n",
        "# original BIN model loading\n",
        "f = load_model('fastText/cc.fa.300.bin')\n",
        "lines=[]\n",
        "\n",
        "# get all words from model\n",
        "words = f.get_words()\n",
        "\n",
        "with open('fastText/cc.fa.300.vec','w') as file_out:\n",
        "    \n",
        "    # the first line must contain number of total words and vector dimension\n",
        "    file_out.write(str(len(words)) + \" \" + str(f.get_dimension()) + \"\\n\")\n",
        "\n",
        "    # line by line, you append vectors to VEC file\n",
        "    for w in words:\n",
        "        v = f.get_word_vector(w)\n",
        "        vstr = \"\"\n",
        "        for vi in v:\n",
        "            vstr += \" \" + str(vi)\n",
        "        try:\n",
        "            file_out.write(w + vstr+'\\n')\n",
        "        except:\n",
        "            pass"
      ],
      "metadata": {
        "id": "RqAd6MmUIgoG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir drive/MyDrive/fastText\n",
        "!cp fastText/cc.fa.300.vec drive/MyDrive/fastText/"
      ],
      "metadata": {
        "id": "yx-MVhUoQ8dO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RUN = 32"
      ],
      "metadata": {
        "id": "Yi3Sdd_lsvMy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir checkpoints"
      ],
      "metadata": {
        "id": "s8AtfVYDtgrw"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    def __init__(self):\n",
        "        # directories\n",
        "        self.train_data_dir = 'data/test_clean.tsv'\n",
        "        self.model_dir = 'models'\n",
        "        self.we_model_dir = 'fastText/cc.fa.300.vec'\n",
        "        self.we_pickled_model_dir = 'fastText/cc.fa.300.pickle'\n",
        "\n",
        "        # general\n",
        "        self.data_split = .1\n",
        "        self.num_epochs = 25\n",
        "        self.batch_size = 16\n",
        "        self.shuffle_buffer = 320000\n",
        "        self.num_tags = 5\n",
        "        self.num_pos_tags = 33\n",
        "        self.word_max_len = 30\n",
        "        self.learning_rate = 1e-3\n",
        "        self.max_len = 1276\n",
        "\n",
        "        # embeddings\n",
        "        self.num_words = 100000\n",
        "        self.word_embed_dim = 300\n",
        "        self.num_chars = 256  # number of most frequent characters to be kept\n",
        "        self.char_embed_dim = 32\n",
        "        self.pos_embed_dim = 16\n",
        "\n",
        "        # lstm variables\n",
        "        self.lstm_units = 256  # number of hidden units in the RNN\n",
        "        self.dropout = .5  # keeping probability"
      ],
      "metadata": {
        "id": "J3PlM7ZmJq1N"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "WcnAZYHytnKr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "import pickle\n",
        "from collections import Counter\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "\n",
        "cfg = Config()\n",
        "\n",
        "class KasreDS(Dataset):\n",
        "    def __init__(self, cfg):\n",
        "        # loading word embedding model\n",
        "        try:\n",
        "            handle = open(cfg.we_pickled_model_dir, 'rb') \n",
        "            self.word_embedding_model = pickle.load(handle)\n",
        "        except FileNotFoundError:\n",
        "            self.word_embedding_model = KeyedVectors.load_word2vec_format(cfg.we_model_dir, binary=False)\n",
        "            with open(cfg.we_pickled_model_dir, 'wb') as handle:\n",
        "                pickle.dump(self.word_embedding_model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "        sents, all_pos_tags, all_ezafe_tags = self._data_reader(cfg.train_data_dir)\n",
        "        print(len(sents))\n",
        "        sents_shuf = []\n",
        "        all_pos_tags_shuf = []\n",
        "        all_ezafe_tags_shuf = []\n",
        "        index_shuf = list(range(len(sents)))\n",
        "\n",
        "        for i in index_shuf:\n",
        "            sents_shuf.append(sents[i])\n",
        "            all_pos_tags_shuf.append(all_pos_tags[i])\n",
        "            all_ezafe_tags_shuf.append(all_ezafe_tags[i])\n",
        "\n",
        "        random.seed(17)\n",
        "        random.shuffle(index_shuf)\n",
        "        \n",
        "        self.sents_shuf = sents_shuf\n",
        "        print('data:', len(self.sents_shuf[0]))\n",
        "        self.data = sents_shuf\n",
        "        sents, all_pos_tags, all_ezafe_tags = self._data_reader(cfg.train_data_dir)\n",
        "        self.sents = sents\n",
        "        self.all_pos_tags = all_pos_tags\n",
        "        self.all_ezafe_tags = all_ezafe_tags\n",
        "\n",
        "        \n",
        "        try:\n",
        "              with open('indices.pickle', 'rb')  as handle:\n",
        "                self.char_to_index, self.word_to_index, self.pos_tag_to_index, self.ezafe_tag_to_index = pickle.load(handle)\n",
        "                \n",
        "                print(self.pos_tag_to_index)\n",
        "\n",
        "                self.index_to_word = {i: key for key, i in self.word_to_index.items()}\n",
        "                self.index_to_ezafe_tag = {i: key for key, i in self.ezafe_tag_to_index.items()}\n",
        "          \n",
        "              \n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print('Building vocabulary...')\n",
        "\n",
        "            vocab_list = []\n",
        "            char_list = []\n",
        "            for sent in self.data:\n",
        "                for word in sent:\n",
        "                    vocab_list.append(word)\n",
        "                    for char in word:\n",
        "                        char_list.append(char)\n",
        "            \n",
        "            most_common_words = Counter(vocab_list).most_common(cfg.num_words)\n",
        "            most_common_chars = Counter(char_list).most_common(cfg.num_chars)\n",
        "            \n",
        "            self.word_to_index = {}\n",
        "            for i, pair in enumerate([('<PAD>', 0)] + most_common_words):\n",
        "                self.word_to_index[pair[0]] = i + 1\n",
        "\n",
        "            self.char_to_index = {}\n",
        "            for i, pair in enumerate([('<PAD>', 0), ('<UNK>', 1)] + most_common_chars):\n",
        "                self.char_to_index[pair[0]] = i + 1\n",
        "            \n",
        "            self.pos_tag_to_index = {}\n",
        "            for i, tag in enumerate(set(x for y in self.data[1] for x in y)):\n",
        "                self.pos_tag_to_index[tag] = i + 1\n",
        "\n",
        "            self.ezafe_tag_to_index = {'O': 0, 'e': 1,'ye': 2, 've':3, 'y':4}\n",
        "\n",
        "            self.index_to_word = {i: key for key, i in self.word_to_index.items()}\n",
        "            self.index_to_ezafe_tag = {i: key for key, i in self.ezafe_tag_to_index.items()}\n",
        "\n",
        "            # saving the tokenizers\n",
        "            with open('indices.pickle', 'wb') as handle:\n",
        "                indices = self.char_to_index, self.word_to_index, self.pos_tag_to_index, self.ezafe_tag_to_index\n",
        "                pickle.dump(indices, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "    def _data_reader(self, directory):\n",
        "        sents, sent = [], []\n",
        "        all_ezafe_tags, ezafe_tags = [], []\n",
        "        all_pos_tags, pos_tags = [], []\n",
        "        with open(directory) as bijankhan_corpus:\n",
        "            for line in bijankhan_corpus:\n",
        "                if line != '\\n':\n",
        "                    word, pos_tag, ezafe_tag = line.strip().split('\\t')\n",
        "                    sent.append(word.replace('ي', 'ی').replace('ك', 'ک').replace('ة', 'ه'))\n",
        "                    pos_tags.append(pos_tag)\n",
        "                    ezafe_tags.append(ezafe_tag)\n",
        "                else:\n",
        "                    if len(sent)>1 :\n",
        "                      sents.append(sent)\n",
        "                      all_pos_tags.append(pos_tags)\n",
        "                      all_ezafe_tags.append(ezafe_tags)\n",
        "                     \n",
        "                    sent = []\n",
        "                    pos_tags = []\n",
        "                    ezafe_tags = []\n",
        "\n",
        "        return sents, all_pos_tags, all_ezafe_tags\n",
        "\t\n",
        "\n",
        "    def _pad(self, word):\n",
        "        for _ in range(cfg.word_max_len - len(word)):\n",
        "            word.append(0)\n",
        "        return word\n",
        "    \n",
        "\n",
        "    def _sent_to_index(self, sentence, mode='word'):\n",
        "        if mode is 'word':\n",
        "            return [self.word_to_index.get(word, 1) for word in sentence]\n",
        "        elif mode is 'char':\n",
        "            indexed_sentence = []\n",
        "            for word in sentence:\n",
        "                indexed_word = []\n",
        "                for char in word:\n",
        "                    indexed_word.append(self.char_to_index.get(word, 1))\n",
        "                indexed_sentence.append(self._pad(indexed_word))\n",
        "            return indexed_sentence\n",
        "\n",
        "\n",
        "    def _sent_to_embed(self, sentence):\n",
        "        embed_sent = []\n",
        "        for word in sentence:\n",
        "            try:\n",
        "                embed_sent.append(self.word_embedding_model[word])\n",
        "            except KeyError:\n",
        "                embed_sent.append([0 for _ in range(cfg.word_embed_dim)])\n",
        "        return embed_sent\n",
        "\n",
        "    \n",
        "    def _pos_tags_to_index(self, tags):\n",
        "        return [self.pos_tag_to_index[tag] for tag in tags]\n",
        "\n",
        "    \n",
        "    def _ezafe_tags_to_index(self, tags):\n",
        "        return [self.ezafe_tag_to_index[tag] for tag in tags]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sents)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        char = True\n",
        "        pos=None\n",
        "        sent, pos_tag, ezafe_tag = self.sents[idx], self.all_pos_tags[idx], self.all_ezafe_tags[idx]\n",
        "        sent_char = self._sent_to_index(sent, mode='char')\n",
        "        sent_word = self._sent_to_embed(sent)\n",
        "        length = [1 for _ in range(len(sent))]\n",
        "        tag = self._ezafe_tags_to_index(ezafe_tag)\n",
        "        # if char:\n",
        "        #     return (np.array(sent_word), np.array(sent_char), np.array(length)), np.array(tag)\n",
        "        if char:\n",
        "            return torch.tensor(sent_word), torch.tensor(tag)"
      ],
      "metadata": {
        "id": "1rr0DI6FIs0F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kasre_ds = KasreDS(cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi0XojytXK2z",
        "outputId": "a2b892df-1f5c-440d-9b8f-e5a5051eb9db"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "53412\n",
            "data: 3\n",
            "Building vocabulary...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(kasre_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxrv2eqDg4ME",
        "outputId": "ffcfa841-23bb-4e90-8cb6-ecd0e20d6612"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53412"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "IVPYR9ujIwqk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, val_dataset = torch.utils.data.random_split(kasre_ds,(52000, 1412), generator=torch.Generator().manual_seed(42) )"
      ],
      "metadata": {
        "id": "M9TMY9oVkLbu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUhCr1mEsmn7",
        "outputId": "57ff642f-5bc9-411e-e6c3-46bd143a1630"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=1, shuffle=True, num_workers=0, pin_memory=True, \n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    val_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "FUDmL6Ahkg51"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = {'train': train_loader,\n",
        "               'test': test_loader,\n",
        "               'val': val_loader}"
      ],
      "metadata": {
        "id": "FDmmOi4ptcHq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KasreAdder(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.cfg = cfg\n",
        "    self.lstm = nn.LSTM(cfg.word_embed_dim, cfg.lstm_units // 2,\n",
        "                            num_layers=1, bidirectional=True, batch_first=True)\n",
        "    self.hidden2tag = nn.Linear(cfg.lstm_units, cfg.num_tags)\n",
        "    self.dropout = nn.Dropout()\n",
        "    self.hidden = self.init_hidden(1)\n",
        "\n",
        "  def init_hidden(self, shape):\n",
        "    return (torch.randn(2, 1, self.cfg.lstm_units // 2).to(device),\n",
        "            torch.randn(2, 1, self.cfg.lstm_units // 2).to(device))\n",
        "    \n",
        "  def forward(self, x):\n",
        "    # print(f'x shape {x.shape}')\n",
        "    self.hidden = self.init_hidden(x.shape[1])\n",
        "    lstm_out, self.hidden = self.lstm(x, self.hidden,)\n",
        "    # print(f'lstm_out {lstm_out.shape}')\n",
        "    # lstm_out = lstm_out.view(len(x), self.cfg.lstm_units)\n",
        "    lstm_out = self.dropout(lstm_out)\n",
        "    lstm_feats = self.hidden2tag(lstm_out[0])\n",
        "    return lstm_feats\n",
        "    "
      ],
      "metadata": {
        "id": "AYg1NIvEI0D1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = KasreAdder(cfg).to(device)"
      ],
      "metadata": {
        "id": "oblrrmdmmlzq"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr=1e-3\n",
        "epochs = 10\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=6, gamma=0.1)\n",
        "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, threshold=0.05, min_lr=1e-7, verbose=True, factor=0.1)\n",
        "es_patience = 50"
      ],
      "metadata": {
        "id": "ERgU-grwsIQ0"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, criterion, lr_scheduler, dataloaders, num_epochs=10):\n",
        "  model = model.train()\n",
        "  train_loss_history = list()\n",
        "  val_loss_history = list()\n",
        "  best_epoch = {'epoch':0, 'accuracy':0, 'loss':-100.0}\n",
        "  model_path = f'checkpoints/RUN_{RUN}'\n",
        "  os.mkdir(model_path)\n",
        "  for epoch in range(num_epochs):\n",
        "    model = model.train()\n",
        "    pbar = tqdm(dataloaders['train'])\n",
        "    epoch_losses = np.array([])\n",
        "    for (batch, labels) in pbar:\n",
        "      batch = batch.to(device, dtype=torch.float)\n",
        "      labels = labels.to(device, dtype=torch.long)[0]\n",
        "      optimizer.zero_grad()\n",
        "      out = model(batch)\n",
        "      # print(batch.shape)\n",
        "      # print(labels.shape)\n",
        "      # print('*************')\n",
        "      # print(out.shape)\n",
        "      loss = criterion(out, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      epoch_losses = np.append(epoch_losses, loss.item())\n",
        "      pbar.set_description(f'epoch {epoch+1}/{num_epochs},\\\n",
        "                            loss= {epoch_losses.mean():.4f}')\n",
        "    train_loss_history.append(epoch_losses.mean())\n",
        "    # lr_scheduler.step()\n",
        "\n",
        "    # validation\n",
        "    model = model.eval()\n",
        "    with torch.no_grad():\n",
        "      pbar = tqdm(dataloaders['val'])\n",
        "      val_loss = np.array([])\n",
        "      n_samples = 0\n",
        "      n_corrects = 0\n",
        "      for (batch, labels) in pbar:\n",
        "        batch = batch.to(device, dtype=torch.float)\n",
        "        labels = labels.to(device, dtype=torch.long)[0]\n",
        "        # labels = torch.tensor([l for l in labels for _ in range(30)]).to(device, dtype=torch.long)\n",
        "        out = model(batch)\n",
        "        loss = criterion(out, labels)\n",
        "        val_loss = np.append(val_loss, loss.item())\n",
        "        out = nn.Softmax()(out)\n",
        "        out = out.argmax(1)\n",
        "        n_corrects += (out == labels).sum()\n",
        "        n_samples += out.shape[0]\n",
        "      val_acc = n_corrects/n_samples\n",
        "      val_loss = val_loss.mean()\n",
        "      val_loss_history.append(val_loss)\n",
        "      lr_scheduler.step(val_loss)\n",
        "      if val_acc > best_epoch['accuracy']:\n",
        "        best_epoch = {'epoch':epoch, 'accuracy':val_acc, 'loss':val_loss}\n",
        "        best_model_path = os.path.join(model_path, f'best_val_checkpoint.pt')\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "      print(f'epoch {epoch+1}/{num_epochs}, loss:= {val_loss.mean():.4f}, accuracy= {(n_corrects/n_samples):.4f}')\n",
        "      pbar.set_description(f'epoch {epoch+1}/{num_epochs}, loss:= {val_loss.mean():.4f}, accuracy= {(n_corrects/n_samples):.4f}')\n",
        "    if (sorted(val_loss_history[-es_patience:]) == val_loss_history[-es_patience:]) and (len(val_loss_history)>2*es_patience):\n",
        "      break\n",
        "  model.load_state_dict(torch.load(best_model_path))\n",
        "  print(best_epoch)\n",
        "  return model, train_loss_history, val_loss_history"
      ],
      "metadata": {
        "id": "Sv9MhXOjsgqy"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RUN += 1\n",
        "model, loss_history, val_loss_history = train(model, optimizer, criterion, lr_scheduler, dataloaders, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pBFH833tS7D",
        "outputId": "c77422be-0ed3-48a3-da31-1ce41a3a1219"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 1/10,                            loss= 0.1501: 100%|██████████| 52000/52000 [06:40<00:00, 129.95it/s]\n",
            "  0%|          | 0/1412 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "100%|██████████| 1412/1412 [00:03<00:00, 439.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1/10, loss:= 0.0990, accuracy= 0.9661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 2/10,                            loss= 0.0964: 100%|██████████| 52000/52000 [06:49<00:00, 126.91it/s]\n",
            "100%|██████████| 1412/1412 [00:03<00:00, 444.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2/10, loss:= 0.0756, accuracy= 0.9748\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 3/10,                            loss= 0.0809: 100%|██████████| 52000/52000 [06:50<00:00, 126.77it/s]\n",
            "100%|██████████| 1412/1412 [00:03<00:00, 450.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3/10, loss:= 0.0687, accuracy= 0.9773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 4/10,                            loss= 0.0717: 100%|██████████| 52000/52000 [06:42<00:00, 129.15it/s]\n",
            "100%|██████████| 1412/1412 [00:03<00:00, 441.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4/10, loss:= 0.0629, accuracy= 0.9801\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 5/10,                            loss= 0.0661: 100%|██████████| 52000/52000 [06:47<00:00, 127.64it/s]\n",
            "100%|██████████| 1412/1412 [00:03<00:00, 441.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5/10, loss:= 0.0638, accuracy= 0.9805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 6/10,                            loss= 0.0607: 100%|██████████| 52000/52000 [06:41<00:00, 129.58it/s]\n",
            "100%|██████████| 1412/1412 [00:03<00:00, 438.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6/10, loss:= 0.0620, accuracy= 0.9805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 7/10,                            loss= 0.0573: 100%|██████████| 52000/52000 [06:41<00:00, 129.49it/s]\n",
            "100%|██████████| 1412/1412 [00:03<00:00, 448.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7/10, loss:= 0.0597, accuracy= 0.9811\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 8/10,                            loss= 0.0547: 100%|██████████| 52000/52000 [06:42<00:00, 129.10it/s]\n",
            "100%|██████████| 1412/1412 [00:03<00:00, 440.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 8/10, loss:= 0.0611, accuracy= 0.9820\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 9/10,                            loss= 0.0521: 100%|██████████| 52000/52000 [06:41<00:00, 129.41it/s]\n",
            "100%|██████████| 1412/1412 [00:03<00:00, 448.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 9/10, loss:= 0.0555, accuracy= 0.9822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 10/10,                            loss= 0.0496: 100%|██████████| 52000/52000 [06:43<00:00, 128.97it/s]\n",
            "100%|██████████| 1412/1412 [00:03<00:00, 433.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 10/10, loss:= 0.0569, accuracy= 0.9819\n",
            "{'epoch': 8, 'accuracy': tensor(0.9822, device='cuda:0'), 'loss': 0.05554357528878405}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent = 'شهر زیبای تهران'.split()\n",
        "sent_words = kasre_ds._sent_to_embed(sent)\n",
        "sent_words = torch.tensor(sent_words).to(device)\n",
        "sent_words.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCprxrkE3HGT",
        "outputId": "2620ca04-b949-4081-a510-d46c2819f275"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 300])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  out = model(sent_words.unsqueeze(0))\n"
      ],
      "metadata": {
        "id": "DCK59Dn6tjOH"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3d5ANjd3zbE",
        "outputId": "bf1b9e3b-95b6-47f0-abfe-abf2f59860ac"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = nn.Softmax()(out)\n",
        "out = out.argmax(1)\n",
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdNeBoBH4PcX",
        "outputId": "7f8fe8db-97a9-4aeb-f21e-05a3859a8190"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 2, 0], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in sent:\n",
        "  print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siJpFk5U4XJn",
        "outputId": "8715f49e-74a5-4dc7-b34b-0ca044d2fd68"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "حال\n",
            "شما\n",
            "چطور\n",
            "است\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5hv8WcjC5cg7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}