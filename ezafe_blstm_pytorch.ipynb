{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ezafe_blstm_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN5koimGVubdyG1+/kgy6ot",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "45d53def1957429abdb64e47b6ecd13a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_903b2904f62b4e71a295ec56f340251d",
              "IPY_MODEL_5506b5c47c484b969b716fe94524fd28",
              "IPY_MODEL_c66c3e4320f947319c8acde77e6bba45"
            ],
            "layout": "IPY_MODEL_7a476e9e1ebc4755827b0f58e762cc94"
          }
        },
        "903b2904f62b4e71a295ec56f340251d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4bbdcd1354a43de8029682cbbe2f725",
            "placeholder": "​",
            "style": "IPY_MODEL_69c832aac92e4757bd3c2b20a779ca40",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json: "
          }
        },
        "5506b5c47c484b969b716fe94524fd28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e38e31ce58a94c5fb08ac55f301b8ede",
            "max": 24459,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd5e34dfd24b40c88dc93b0baabc3880",
            "value": 24459
          }
        },
        "c66c3e4320f947319c8acde77e6bba45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6218491732eb48068e8aeec1270725b1",
            "placeholder": "​",
            "style": "IPY_MODEL_6361390312c44c4e8fe5a07ae3cb231d",
            "value": " 142k/? [00:00&lt;00:00, 2.12MB/s]"
          }
        },
        "7a476e9e1ebc4755827b0f58e762cc94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4bbdcd1354a43de8029682cbbe2f725": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69c832aac92e4757bd3c2b20a779ca40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e38e31ce58a94c5fb08ac55f301b8ede": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd5e34dfd24b40c88dc93b0baabc3880": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6218491732eb48068e8aeec1270725b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6361390312c44c4e8fe5a07ae3cb231d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahdiEsrafili/pert/blob/master/ezafe_blstm_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fk2oqX4CHbHt",
        "outputId": "e65abd3b-c75c-4d73-abde-8d54e8d29083"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnebXvZ2HiRB",
        "outputId": "e904d4d6-237b-40f5-af29-b0f437046264"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stanza\n",
            "  Downloading stanza-1.3.0-py3-none-any.whl (432 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 27.5 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30 kB 16.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 15.4 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 51 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 61 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |██████                          | 81 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 102 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 112 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 122 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 143 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 153 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 163 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 174 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 184 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 194 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 204 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 215 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 225 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 235 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 245 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 256 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 266 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 276 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 286 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 296 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 307 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 317 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 327 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 337 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 348 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 358 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 368 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 378 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 389 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 399 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 409 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 419 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 430 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 432 kB 9.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stanza) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from stanza) (1.10.0+cu111)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from stanza) (1.15.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanza) (3.17.3)\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 47.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stanza) (1.21.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->stanza) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (1.24.3)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=e9e6acb4037c59c4675b2d0a90152b0da35fcd432d71c382fee4f8042d0eb56f\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji, stanza\n",
            "Successfully installed emoji-1.7.0 stanza-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "stanza.download('fa')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132,
          "referenced_widgets": [
            "45d53def1957429abdb64e47b6ecd13a",
            "903b2904f62b4e71a295ec56f340251d",
            "5506b5c47c484b969b716fe94524fd28",
            "c66c3e4320f947319c8acde77e6bba45",
            "7a476e9e1ebc4755827b0f58e762cc94",
            "c4bbdcd1354a43de8029682cbbe2f725",
            "69c832aac92e4757bd3c2b20a779ca40",
            "e38e31ce58a94c5fb08ac55f301b8ede",
            "fd5e34dfd24b40c88dc93b0baabc3880",
            "6218491732eb48068e8aeec1270725b1",
            "6361390312c44c4e8fe5a07ae3cb231d"
          ]
        },
        "id": "K67Qbr_4Hs3b",
        "outputId": "6459b2e3-7ce4-43b9-b89d-e8bf42c0641d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45d53def1957429abdb64e47b6ecd13a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-18 17:54:53 INFO: Downloading default packages for language: fa (Persian)...\n",
            "2022-04-18 17:54:54 INFO: File exists: /root/stanza_resources/fa/default.zip.\n",
            "2022-04-18 17:54:57 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_pos = stanza.Pipeline(lang='fa', processors='tokenize,mwt,pos')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vY9qy_5sIAw_",
        "outputId": "b5d6b803-0e62-42a8-a557-dd9c3bc66606"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-18 17:54:57 INFO: Loading these models for language: fa (Persian):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | perdt   |\n",
            "| mwt       | perdt   |\n",
            "| pos       | perdt   |\n",
            "=======================\n",
            "\n",
            "2022-04-18 17:54:57 INFO: Use device: gpu\n",
            "2022-04-18 17:54:57 INFO: Loading: tokenize\n",
            "2022-04-18 17:55:06 INFO: Loading: mwt\n",
            "2022-04-18 17:55:06 INFO: Loading: pos\n",
            "2022-04-18 17:55:07 INFO: Done loading processors!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/fastText.git\n",
        "% cd fastText\n",
        "!  pip install .\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EloiwuwfIB7P",
        "outputId": "378b6830-5ef9-4b1f-8b85-3a6a8d6f7fe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fastText'...\n",
            "remote: Enumerating objects: 3930, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 3930 (delta 28), reused 42 (delta 11), pack-reused 3854\u001b[K\n",
            "Receiving objects: 100% (3930/3930), 8.33 MiB | 38.27 MiB/s, done.\n",
            "Resolving deltas: 100% (2445/2445), done.\n",
            "/content/fastText\n",
            "Processing /content/fastText\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.9.2-py2.py3-none-any.whl (213 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (1.21.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3139533 sha256=98cc7ca5d1af18910d7b21d5387950286ea748c6c5a497c0ee1f807e09b06f4c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9rxqqt_f/wheels/22/04/6e/b3aba25c1a5845898b5871a0df37c2126cb0cc9326ad0c08e7\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.9.2\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "% cd fastText\n",
        "!./download_model.py fa\n",
        "% cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Fw2SkiaID03",
        "outputId": "91fd7afe-094e-4a2d-94e5-11b3a8ca032d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/fastText\n",
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz\n",
            " (100.00%) [==================================================>]\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data"
      ],
      "metadata": {
        "id": "1GvRlOd7IFwn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run this or below cell\n",
        "!cp drive/MyDrive/دیتاست\\ بیجن\\ خان/test_clean.tsv data/test_clean.tsv\n",
        "!cp drive/MyDrive/دیتاست\\ بیجن\\ خان/train_clean.tsv data/train_clean.tsv"
      ],
      "metadata": {
        "id": "rtOY4RPkOrtr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wc -l data/train_clean.tsv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NL27nT6e079M",
        "outputId": "c998a9eb-184c-461a-fee9-91f9d270f7a5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9049352 data/train_clean.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wc -l data/test_clean.tsv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-Tk7enI1HvL",
        "outputId": "750ad0b1-895c-4323-e653-94a88cf5a870"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1601230 data/test_clean.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n4000000 data/train_clean.tsv > data/train_clean_p1.tsv"
      ],
      "metadata": {
        "id": "2vMw5h2D0FZu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wc -l data/train_clean_p1.tsv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6oZsQ2y1Moj",
        "outputId": "109d7547-4bf1-4b45-90a4-171d36f0eb22"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4000000 data/train_clean_p1.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip drive/MyDrive/دیتاست\\ بیجن\\ خان/test_data.zip -d data\n",
        "!unzip drive/MyDrive/دیتاست\\ بیجن\\ خان/train_data.zip -d data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPeQzq74III-",
        "outputId": "09315c2e-3643-414d-d395-afc21add915b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  drive/MyDrive/دیتاست بیجن خان/test_data.zip\n",
            "  inflating: data/test_data.txt      \n",
            "Archive:  drive/MyDrive/دیتاست بیجن خان/train_data.zip\n",
            "  inflating: data/train_data.txt     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_pos = stanza.Pipeline(lang='fa', processors='tokenize,mwt,pos')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IByGIyi0IMLm",
        "outputId": "29d468b9-9e19-4737-be49-7c9bc9f76f2a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-18 17:57:26 INFO: Loading these models for language: fa (Persian):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | perdt   |\n",
            "| mwt       | perdt   |\n",
            "| pos       | perdt   |\n",
            "=======================\n",
            "\n",
            "2022-04-18 17:57:26 INFO: Use device: gpu\n",
            "2022-04-18 17:57:26 INFO: Loading: tokenize\n",
            "2022-04-18 17:57:26 INFO: Loading: mwt\n",
            "2022-04-18 17:57:26 INFO: Loading: pos\n",
            "2022-04-18 17:57:27 INFO: Done loading processors!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def pos_tager(text):\n",
        "  doc = nlp_pos(text)\n",
        "  pos = [word.xpos for sent in doc.sentences for word in sent.words if word.text not in ['ش','شان','م', 'مان', 'ند','ست','یت', 'تان', 'ت','اش']]\n",
        "  # pos_ = [(word.text , word.upos, word.xpos) for sent in doc.sentences for word in sent.words ]\n",
        "  # print(pos_)\n",
        "  return pos"
      ],
      "metadata": {
        "id": "TGxCnhOfIO4W"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "r8ZQFOE_IP13"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_consist(pos_tag, kasre_tag, sent):\n",
        "  l1 = len(pos_tag)\n",
        "  l2 = len(kasre_tag)\n",
        "  l3 = len(sent.split())\n",
        "  if l1!=l2:\n",
        "    return False\n",
        "  if l1!=l3:\n",
        "    return False\n",
        "  if l3!=l2:\n",
        "    return False\n",
        "  return True"
      ],
      "metadata": {
        "id": "I70JzE8PIUg5"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data/train_data.txt', encoding='utf-8') as f:\n",
        "    data = f.readlines()\n",
        "sents = list()\n",
        "kasre_tags = list()\n",
        "temp_sent = ''\n",
        "temp_tags = list()\n",
        "pos_tags = list()\n",
        "for i, line in tqdm(enumerate(data)):\n",
        "    # if i>10000: break\n",
        "    try:\n",
        "        word, ez = line.split()\n",
        "        word += ' '\n",
        "        temp_sent += word\n",
        "        temp_tags.append(ez)\n",
        "        if word in ['. ', '# ']:\n",
        "            pos_tag = pos_tager(temp_sent)\n",
        "            ok = check_consist(pos_tag, temp_tags, temp_sent)\n",
        "            if ok:\n",
        "              sents.append(temp_sent) \n",
        "              kasre_tags.append(temp_tags)\n",
        "              pos_tags.append(pos_tag)\n",
        "            temp_sent = ''\n",
        "            temp_tags = list()\n",
        "    except:\n",
        "        pos_tag = pos_tager(temp_sent)\n",
        "        ok = check_consist(pos_tag, temp_tags, temp_sent)\n",
        "        if ok:\n",
        "          sents.append(temp_sent) \n",
        "          kasre_tags.append(temp_tags)\n",
        "          pos_tags.append(pos_tag)\n",
        "        temp_sent = ''\n",
        "        temp_tags = list()   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrWMK6ZPIVaF",
        "outputId": "374e7f34-a84d-45eb-8abd-37c67ba35b04"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "130it [00:00, 517.04it/s]/usr/local/lib/python3.7/dist-packages/stanza/models/common/beam.py:86: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  prevK = bestScoresId // numWords\n",
            "8612838it [1:46:31, 1347.55it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sents), len(kasre_tags), len(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWuilPW2IYWW",
        "outputId": "3e989c40-07c8-404c-d2a6-aa8cda7fda71"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(295721, 295721, 295721)"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raws = list()\n",
        "\n",
        "for i in range(len(pos_tags)):\n",
        "  for j in range(len(pos_tags[i])):\n",
        "    raw = f'{sents[i].split()[j]}\\t{pos_tags[i][j]}\\t{kasre_tags[i][j]}'\n",
        "    raws.append(raw)\n",
        "  raws.append('\\n')"
      ],
      "metadata": {
        "id": "IyetfYuTIdCu"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raws_text = '\\n'.join(raws)\n",
        "with open('data/train_clean.tsv', 'w') as f:\n",
        "  f.write(raws_text)"
      ],
      "metadata": {
        "id": "Bw0bUhwYIeTW"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp data/train_clean.tsv drive/MyDrive/دیتاست\\ بیجن\\ خان/"
      ],
      "metadata": {
        "id": "6uJsLaiROct1"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run this or bellow cell\n",
        "!mkdir fastText\n",
        "!cp drive/MyDrive/fastText/cc.fa.300.vec fastText/"
      ],
      "metadata": {
        "id": "q-mzML_PRHLU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fasttext import load_model\n",
        "\n",
        "# original BIN model loading\n",
        "f = load_model('fastText/cc.fa.300.bin')\n",
        "lines=[]\n",
        "\n",
        "# get all words from model\n",
        "words = f.get_words()\n",
        "\n",
        "with open('fastText/cc.fa.300.vec','w') as file_out:\n",
        "    \n",
        "    # the first line must contain number of total words and vector dimension\n",
        "    file_out.write(str(len(words)) + \" \" + str(f.get_dimension()) + \"\\n\")\n",
        "\n",
        "    # line by line, you append vectors to VEC file\n",
        "    for w in words:\n",
        "        v = f.get_word_vector(w)\n",
        "        vstr = \"\"\n",
        "        for vi in v:\n",
        "            vstr += \" \" + str(vi)\n",
        "        try:\n",
        "            file_out.write(w + vstr+'\\n')\n",
        "        except:\n",
        "            pass"
      ],
      "metadata": {
        "id": "RqAd6MmUIgoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir drive/MyDrive/fastText\n",
        "!cp fastText/cc.fa.300.vec drive/MyDrive/fastText/"
      ],
      "metadata": {
        "id": "yx-MVhUoQ8dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir checkpoints"
      ],
      "metadata": {
        "id": "s8AtfVYDtgrw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RUN = 0"
      ],
      "metadata": {
        "id": "Yi3Sdd_lsvMy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    def __init__(self):\n",
        "        # directories\n",
        "        self.train_data_dir = 'data/train_clean_p1.tsv'\n",
        "        self.model_dir = 'models'\n",
        "        self.we_model_dir = 'fastText/cc.fa.300.vec'\n",
        "        self.we_pickled_model_dir = 'fastText/cc.fa.300.pickle'\n",
        "\n",
        "        # general\n",
        "        self.data_split = .1\n",
        "        self.num_epochs = 25\n",
        "        self.batch_size = 16\n",
        "        self.shuffle_buffer = 320000\n",
        "        self.num_tags = 6\n",
        "        self.num_pos_tags = 33\n",
        "        self.word_max_len = 30\n",
        "        self.learning_rate = 1e-3\n",
        "        self.max_len = 100\n",
        "\n",
        "        # embeddings\n",
        "        self.num_words = 100000\n",
        "        self.word_embed_dim = 300\n",
        "        self.num_chars = 256  # number of most frequent characters to be kept\n",
        "        self.char_embed_dim = 32\n",
        "        self.pos_embed_dim = 16\n",
        "\n",
        "        # lstm variables\n",
        "        self.lstm_units = 256  # number of hidden units in the RNN\n",
        "        self.dropout = .5  # keeping probability"
      ],
      "metadata": {
        "id": "J3PlM7ZmJq1N"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "WcnAZYHytnKr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "import pickle\n",
        "from collections import Counter\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "from torch.nn.functional import one_hot\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "cfg = Config()\n",
        "\n",
        "class KasreDS(Dataset):\n",
        "    def __init__(self, cfg):\n",
        "        # loading word embedding model\n",
        "        try:\n",
        "            handle = open(cfg.we_pickled_model_dir, 'rb') \n",
        "            self.word_embedding_model = pickle.load(handle)\n",
        "        except FileNotFoundError:\n",
        "            self.word_embedding_model = KeyedVectors.load_word2vec_format(cfg.we_model_dir, binary=False)\n",
        "            with open(cfg.we_pickled_model_dir, 'wb') as handle:\n",
        "                pickle.dump(self.word_embedding_model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "        sents, all_pos_tags, all_ezafe_tags = self._data_reader(cfg.train_data_dir)\n",
        "        print(len(sents))\n",
        "        sents_shuf = []\n",
        "        all_pos_tags_shuf = []\n",
        "        all_ezafe_tags_shuf = []\n",
        "        index_shuf = list(range(len(sents)))\n",
        "\n",
        "        for i in index_shuf:\n",
        "            sents_shuf.append(sents[i])\n",
        "            all_pos_tags_shuf.append(all_pos_tags[i])\n",
        "            all_ezafe_tags_shuf.append(all_ezafe_tags[i])\n",
        "\n",
        "        random.seed(17)\n",
        "        random.shuffle(index_shuf)\n",
        "        \n",
        "        self.sents_shuf = sents_shuf\n",
        "        print('data:', len(self.sents_shuf[0]))\n",
        "        self.data = sents_shuf\n",
        "        sents, all_pos_tags, all_ezafe_tags = self._data_reader(cfg.train_data_dir)\n",
        "        self.sents = sents\n",
        "        self.all_pos_tags = all_pos_tags\n",
        "        self.all_ezafe_tags = all_ezafe_tags\n",
        "\n",
        "        \n",
        "        try:\n",
        "              with open('indices.pickle', 'rb')  as handle:\n",
        "                self.char_to_index, self.word_to_index, self.pos_tag_to_index, self.ezafe_tag_to_index = pickle.load(handle)\n",
        "                \n",
        "                print(self.pos_tag_to_index)\n",
        "\n",
        "                self.index_to_word = {i: key for key, i in self.word_to_index.items()}\n",
        "                self.index_to_ezafe_tag = {i: key for key, i in self.ezafe_tag_to_index.items()}\n",
        "          \n",
        "              \n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print('Building vocabulary...')\n",
        "\n",
        "            vocab_list = []\n",
        "            char_list = []\n",
        "            for sent in self.data:\n",
        "                for word in sent:\n",
        "                    vocab_list.append(word)\n",
        "                    for char in word:\n",
        "                        char_list.append(char)\n",
        "            \n",
        "            most_common_words = Counter(vocab_list).most_common(cfg.num_words)\n",
        "            most_common_chars = Counter(char_list).most_common(cfg.num_chars)\n",
        "            \n",
        "            self.word_to_index = {}\n",
        "            for i, pair in enumerate([('<PAD>', 0)] + most_common_words):\n",
        "                self.word_to_index[pair[0]] = i + 1\n",
        "\n",
        "            self.char_to_index = {}\n",
        "            for i, pair in enumerate([('<PAD>', 0), ('<UNK>', 1)] + most_common_chars):\n",
        "                self.char_to_index[pair[0]] = i + 1\n",
        "            \n",
        "            self.pos_tag_to_index = {}\n",
        "            for i, tag in enumerate(set(x for y in self.all_pos_tags for x in y)):\n",
        "                self.pos_tag_to_index[tag] = i + 1\n",
        "\n",
        "            self.ezafe_tag_to_index = {'O': 0, 'e': 1,'ye': 2, 've':3, 'y':4, '@e':5}\n",
        "\n",
        "            self.index_to_word = {i: key for key, i in self.word_to_index.items()}\n",
        "            self.index_to_ezafe_tag = {i: key for key, i in self.ezafe_tag_to_index.items()}\n",
        "\n",
        "            # saving the tokenizers\n",
        "            with open('indices.pickle', 'wb') as handle:\n",
        "                indices = self.char_to_index, self.word_to_index, self.pos_tag_to_index, self.ezafe_tag_to_index\n",
        "                pickle.dump(indices, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "    def _data_reader(self, directory):\n",
        "        sents, sent = [], []\n",
        "        all_ezafe_tags, ezafe_tags = [], []\n",
        "        all_pos_tags, pos_tags = [], []\n",
        "        with open(directory) as bijankhan_corpus:\n",
        "            for line in bijankhan_corpus:\n",
        "                if line != '\\n':\n",
        "                    word, pos_tag, ezafe_tag = line.strip().split('\\t')\n",
        "                    sent.append(word.replace('ي', 'ی').replace('ك', 'ک').replace('ة', 'ه'))\n",
        "                    pos_tags.append(pos_tag)\n",
        "                    ezafe_tags.append(ezafe_tag)\n",
        "                else:\n",
        "                    if len(sent)>1 :\n",
        "                      sents.append(sent)\n",
        "                      all_pos_tags.append(pos_tags)\n",
        "                      all_ezafe_tags.append(ezafe_tags)\n",
        "                     \n",
        "                    sent = []\n",
        "                    pos_tags = []\n",
        "                    ezafe_tags = []\n",
        "\n",
        "        return sents, all_pos_tags, all_ezafe_tags\n",
        "\t\n",
        "\n",
        "    def _pad(self, word):\n",
        "        for _ in range(cfg.word_max_len - len(word)):\n",
        "            word.append(0)\n",
        "        return word\n",
        "    \n",
        "\n",
        "    def _sent_to_index(self, sentence, mode='word'):\n",
        "        if mode is 'word':\n",
        "            return [self.word_to_index.get(word, 1) for word in sentence]\n",
        "        elif mode is 'char':\n",
        "            indexed_sentence = []\n",
        "            for word in sentence:\n",
        "                indexed_word = []\n",
        "                for char in word:\n",
        "                    indexed_word.append(self.char_to_index.get(word, 1))\n",
        "                indexed_sentence.append(self._pad(indexed_word))\n",
        "            return indexed_sentence\n",
        "\n",
        "\n",
        "    def _sent_to_embed(self, sentence):\n",
        "        embed_sent = []\n",
        "        for word in sentence:\n",
        "            try:\n",
        "                embed_sent.append(self.word_embedding_model[word])\n",
        "            except KeyError:\n",
        "                embed_sent.append([0 for _ in range(cfg.word_embed_dim)])\n",
        "        return embed_sent\n",
        "\n",
        "    \n",
        "    def _pos_tags_to_index(self, tags):\n",
        "        return torch.stack([one_hot(torch.tensor(self.pos_tag_to_index[tag]), num_classes=cfg.num_pos_tags+1) for tag in tags])\n",
        "\n",
        "    \n",
        "    def _ezafe_tags_to_index(self, tags):\n",
        "        return [self.ezafe_tag_to_index[tag] for tag in tags]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sents)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        char = True\n",
        "        pos=None\n",
        "        sent, pos_tag, ezafe_tag = self.sents[idx], self.all_pos_tags[idx], self.all_ezafe_tags[idx]\n",
        "        sent_char = self._sent_to_index(sent, mode='char')\n",
        "        sent_word = self._sent_to_embed(sent)\n",
        "        length = [1 for _ in range(len(sent))]\n",
        "        tag = self._ezafe_tags_to_index(ezafe_tag)\n",
        "        pos_tag = self._pos_tags_to_index(pos_tag)\n",
        "        pos_tag = torch.tensor(pos_tag)\n",
        "        # if char:\n",
        "        #     return (np.array(sent_word), np.array(sent_char), np.array(length)), np.array(tag)\n",
        "        if char:\n",
        "            return torch.tensor(sent_word), pos_tag, torch.tensor(tag)\n"
      ],
      "metadata": {
        "id": "5C3SVZoupGKd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kasre_ds = KasreDS(cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi0XojytXK2z",
        "outputId": "882220ac-9340-45cf-a8c0-4a4312aef418"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "127229\n",
            "data: 23\n",
            "Building vocabulary...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(kasre_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxrv2eqDg4ME",
        "outputId": "412d36d5-905a-4809-f2d3-f1bb46f73603"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "127229"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, x, _ = kasre_ds[0]"
      ],
      "metadata": {
        "id": "IVPYR9ujIwqk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1d514aa-ff77-4e9e-fb1d-828cd12b64b7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:171: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, val_dataset = torch.utils.data.random_split(kasre_ds,(126229, 1000), generator=torch.Generator().manual_seed(42) )"
      ],
      "metadata": {
        "id": "M9TMY9oVkLbu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUhCr1mEsmn7",
        "outputId": "1aea212b-e117-41fd-e5bf-3ec5b39019b8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=1, shuffle=True, num_workers=0, pin_memory=True, \n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    val_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "FUDmL6Ahkg51"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = {'train': train_loader,\n",
        "               'test': test_loader,\n",
        "               'val': val_loader}"
      ],
      "metadata": {
        "id": "FDmmOi4ptcHq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KasreAdder(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.cfg = cfg\n",
        "    self.lstm = nn.LSTM(cfg.word_embed_dim, cfg.lstm_units // 2,\n",
        "                            num_layers=2, bidirectional=True, batch_first=True)\n",
        "    \n",
        "    self.lstm_pos = nn.LSTM(34, 8,\n",
        "                            num_layers=1, bidirectional=True, batch_first=True)\n",
        "    \n",
        "    self.hidden2tag = nn.Linear(cfg.lstm_units+16, cfg.num_tags)\n",
        "    self.dropout = nn.Dropout()\n",
        "    self.hidden = self.init_hidden()\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return (torch.zeros(4, 1, self.cfg.lstm_units // 2).to(device),\n",
        "            torch.zeros(4, 1, self.cfg.lstm_units // 2).to(device))\n",
        "\n",
        "  def init_hidden_pos(self):\n",
        "    return (torch.zeros(2, 1, 8).to(device),\n",
        "            torch.zeros(2, 1, 8).to(device)) \n",
        "  \n",
        "    \n",
        "  def forward(self, x, pos, hidden=None):\n",
        "    # print(f'x shape {x.shape}')\n",
        "    self.hidden = hidden\n",
        "    if hidden is None:\n",
        "      self.hidden = self.init_hidden()\n",
        "      self.hidden_pos = self.init_hidden_pos()\n",
        "    lstm_out, self.hidden = self.lstm(x, self.hidden,)\n",
        "    lstm_pos_out, self.hidden_pos = self.lstm_pos(pos, self.hidden_pos,)\n",
        "    # print(f'lstm_out {lstm_out.shape}')\n",
        "    # lstm_out = lstm_out.view(len(x), self.cfg.lstm_units)\n",
        "    lstm_out = torch.concat((lstm_out, lstm_pos_out), dim=2)\n",
        "    lstm_out = self.dropout(lstm_out)\n",
        "    lstm_feats = self.hidden2tag(lstm_out[0])\n",
        "    return lstm_feats\n",
        "    "
      ],
      "metadata": {
        "id": "AYg1NIvEI0D1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = KasreAdder(cfg).to(device)"
      ],
      "metadata": {
        "id": "oblrrmdmmlzq"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.load_state_dict(torch.load('drive/MyDrive/kasre_model.pt'))"
      ],
      "metadata": {
        "id": "tijkR5ge2l5h"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr=1e-3\n",
        "epochs = 2\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=6, gamma=0.1)\n",
        "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, threshold=0.05, min_lr=1e-7, verbose=True, factor=0.1)\n",
        "es_patience = 50"
      ],
      "metadata": {
        "id": "ERgU-grwsIQ0"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, criterion, lr_scheduler, dataloaders, num_epochs=10):\n",
        "  model = model.train()\n",
        "  train_loss_history = list()\n",
        "  val_loss_history = list()\n",
        "  best_epoch = {'epoch':0, 'accuracy':0, 'loss':-100.0}\n",
        "  model_path = f'checkpoints/RUN_{RUN}'\n",
        "  os.mkdir(model_path)\n",
        "  for epoch in range(num_epochs):\n",
        "    model = model.train()\n",
        "    pbar = tqdm(dataloaders['train'])\n",
        "    epoch_losses = np.array([])\n",
        "    for (batch, pos_batch, labels) in pbar:\n",
        "      batch = batch.to(device, dtype=torch.float)\n",
        "      pos_batch = pos_batch.to(device, dtype=torch.float)\n",
        "      labels = labels.to(device, dtype=torch.long)[0]\n",
        "      optimizer.zero_grad()\n",
        "      out = model(batch, pos_batch)\n",
        "      # print(batch.shape)\n",
        "      # print(labels.shape)\n",
        "      # print('*************')\n",
        "      # print(out.shape)\n",
        "      loss = criterion(out, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      epoch_losses = np.append(epoch_losses, loss.item())\n",
        "      pbar.set_description(f'epoch {epoch+1}/{num_epochs},\\\n",
        "                            loss= {epoch_losses.mean():.4f}')\n",
        "    train_loss_history.append(epoch_losses.mean())\n",
        "    # lr_scheduler.step()\n",
        "\n",
        "    # validation\n",
        "    model = model.eval()\n",
        "    with torch.no_grad():\n",
        "      pbar = tqdm(dataloaders['val'])\n",
        "      val_loss = np.array([])\n",
        "      n_samples = 0\n",
        "      n_corrects = 0\n",
        "      for (batch, pos_batch, labels) in pbar:\n",
        "        batch = batch.to(device, dtype=torch.float)\n",
        "        pos_batch = pos_batch.to(device, dtype=torch.float)\n",
        "        labels = labels.to(device, dtype=torch.long)[0]\n",
        "        # labels = torch.tensor([l for l in labels for _ in range(30)]).to(device, dtype=torch.long)\n",
        "        out = model(batch, pos_batch)\n",
        "        loss = criterion(out, labels)\n",
        "        val_loss = np.append(val_loss, loss.item())\n",
        "        out = nn.Softmax()(out)\n",
        "        out = out.argmax(1)\n",
        "        n_corrects += (out == labels).sum()\n",
        "        n_samples += out.shape[0]\n",
        "      val_acc = n_corrects/n_samples\n",
        "      val_loss = val_loss.mean()\n",
        "      val_loss_history.append(val_loss)\n",
        "      lr_scheduler.step(val_loss)\n",
        "      if val_acc > best_epoch['accuracy']:\n",
        "        best_epoch = {'epoch':epoch, 'accuracy':val_acc, 'loss':val_loss}\n",
        "        best_model_path = os.path.join(model_path, f'best_val_checkpoint.pt')\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "      print(f'epoch {epoch+1}/{num_epochs}, loss:= {val_loss.mean():.4f}, accuracy= {(n_corrects/n_samples):.4f}')\n",
        "      pbar.set_description(f'epoch {epoch+1}/{num_epochs}, loss:= {val_loss.mean():.4f}, accuracy= {(n_corrects/n_samples):.4f}')\n",
        "    if (sorted(val_loss_history[-es_patience:]) == val_loss_history[-es_patience:]) and (len(val_loss_history)>2*es_patience):\n",
        "      break\n",
        "  model.load_state_dict(torch.load(best_model_path))\n",
        "  print(best_epoch)\n",
        "  return model, train_loss_history, val_loss_history"
      ],
      "metadata": {
        "id": "Sv9MhXOjsgqy"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RUN += 1\n",
        "model, loss_history, val_loss_history = train(model, optimizer, criterion, lr_scheduler, dataloaders, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pBFH833tS7D",
        "outputId": "4c7ede37-40fe-4fee-84a0-76a125c74a99"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/126229 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:171: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:175: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "epoch 1/2,                            loss= 0.0805: 100%|██████████| 126229/126229 [37:49<00:00, 55.63it/s]\n",
            "  0%|          | 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "100%|██████████| 1000/1000 [00:05<00:00, 173.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1/2, loss:= 0.0573, accuracy= 0.9821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 2/2,                            loss= 0.0475: 100%|██████████| 126229/126229 [37:41<00:00, 55.81it/s]\n",
            "100%|██████████| 1000/1000 [00:05<00:00, 174.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2/2, loss:= 0.0493, accuracy= 0.9855\n",
            "{'epoch': 1, 'accuracy': tensor(0.9855, device='cuda:0'), 'loss': 0.04928675359127169}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RUN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPXYSGkDSWJH",
        "outputId": "60f36f85-efba-4772-efc5-677cc7e9915c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp checkpoints/RUN_1/best_val_checkpoint.pt drive/MyDrive/kasre_pos_model_big.pt"
      ],
      "metadata": {
        "id": "bOxae36F5gdV"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('drive/MyDrive/kasre_model.pt'))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "Xb8oVPlZTYrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device).eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdNeBoBH4PcX",
        "outputId": "e297e4fd-92e1-445d-cd89-8983dbe7fc90"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KasreAdder(\n",
              "  (lstm): LSTM(300, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
              "  (lstm_pos): LSTM(34, 8, batch_first=True, bidirectional=True)\n",
              "  (hidden2tag): Linear(in_features=272, out_features=6, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tag2kasre =  {0:'',\n",
        "              1:'ِ',\n",
        "              2:'‌یِ',\n",
        "              3:'ِ',\n",
        "              4:'ی',\n",
        "              5:'ِ'}\n",
        "def add_kasre(sent):\n",
        "  sent_ = sent.split()\n",
        "  sent_words = kasre_ds._sent_to_embed(sent_)\n",
        "  sent_words = torch.tensor(sent_words).float().to(device)\n",
        "  pos = pos_tager(sent)\n",
        "  pos = kasre_ds._pos_tags_to_index(pos)\n",
        "  pos = torch.tensor(pos).float().to(device)\n",
        "  print(f'sent shape {sent_words.shape}, pos shape {pos.shape}')\n",
        "  with torch.no_grad():\n",
        "    out = model(sent_words.unsqueeze(0), pos.unsqueeze(0))\n",
        "    out = nn.Softmax()(out)\n",
        "    out = out.argmax(1).cpu().numpy()\n",
        "  result = ''\n",
        "  for i,word in enumerate(sent_):\n",
        "    result += word + tag2kasre[out[i]] + ' '\n",
        "  return result\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5hv8WcjC5cg7"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "lF4-e3bxvpxI"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "print(add_kasre('این خانه بزرگ در شهر ما واقع است'))\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zV5RDmOqAgV",
        "outputId": "911ae0f3-ed9b-4867-fe46-7a4d9be3031c"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sent shape torch.Size([8, 300]), pos shape torch.Size([8, 34])\n",
            "این خانه‌یِ بزرگ در شهرِ ما واقع است \n",
            "0.03261590003967285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZvT6-C67qDlj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}